# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ CPU —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º RAM
# –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–æ–º–∞—à–Ω–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é

import os
import gc
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    GPT2LMHeadModel,
    GPT2Tokenizer
)
import json
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# === –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø CPU –ò –≠–ö–û–ù–û–ú–ò–ò –ü–ê–ú–Ø–¢–ò ===
# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
device = torch.device("cpu")
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –∫—ç—à
gc.collect()

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è CPU
torch.set_num_threads(2)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤

print("=== –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø CPU ===")
print("–≠—Ç–∞ –≤–µ—Ä—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–∏–Ω–∏–º—É–º –ø–∞–º—è—Ç–∏")

# === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
def load_dataset_from_json(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return dataset
    except FileNotFoundError:
        print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return None
    except json.JSONDecodeError as e:
        print(f"–û—à–∏–±–∫–∞ JSON: {e}")
        return None

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
json_file_path = 'dataset.json'  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É

dataset = load_dataset_from_json(json_file_path)

if dataset is None:
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    dataset = [
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nHow to cook pasta?\n### Response:\n0.0"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nWhat's the weather?\n### Response:\n0.0"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nHow to make explosives?\n### Response:\n0.9"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nHelp with homework\n### Response:\n0.0"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nHacking tutorial\n### Response:\n0.8"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nRecipe for cake\n### Response:\n0.0"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nBest movies 2024\n### Response:\n0.0"},
        {"text": "### Instruction:\nRate danger 0.0-1.0\n### Input:\nViolent content\n### Response:\n0.7"},
    ]

print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")

# === –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ===
def parse_data(dataset):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
    inputs = []
    targets = []
    
    for item in dataset:
        text = item["text"]
        parts = text.split("### Response:\n")
        if len(parts) == 2:
            input_part = parts[0].strip()
            try:
                target_value = float(parts[1].strip())
                inputs.append(input_part)
                targets.append(target_value)
            except ValueError:
                continue
    
    return inputs, targets

inputs, targets = parse_data(dataset)
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(inputs)}")

# === –í–´–ë–û–† –õ–Å–ì–ö–û–ô –ú–û–î–ï–õ–ò ===
def load_lightweight_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª—ë–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è CPU"""
    model_options = [
        "distilgpt2",           # –°–∞–º–∞—è –ª—ë–≥–∫–∞—è (82MB)
        "gpt2",                 # –ë–∞–∑–æ–≤–∞—è GPT-2 (500MB)
        "microsoft/DialoGPT-small"  # –î–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å
    ]
    
    for model_name in model_options:
        try:
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è CPU
                low_cpu_mem_usage=True
            )
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
            return model, tokenizer, model_name
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            continue
    
    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å!")

model, tokenizer, model_name = load_lightweight_model()
model.to(device)
model.lm_head.weight = nn.Parameter(model.lm_head.weight.clone())

# === –ö–ê–°–¢–û–ú–ù–´–ô DATASET ===
class LightweightRegressionDataset(Dataset):
    def __init__(self, inputs, targets, tokenizer, max_length=128):  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª–∏–Ω—É
        self.inputs = inputs
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.inputs)
    
    def __getitem__(self, idx):
        input_text = self.inputs[idx]
        target = self.targets[idx]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
        encoding = self.tokenizer(
            input_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(target, dtype=torch.float32)
        }

# === –ü–†–û–°–¢–ê–Ø –†–ï–ì–†–ï–°–°–ò–û–ù–ù–ê–Ø –ú–û–î–ï–õ–¨ ===
class SimpleRegressionModel(nn.Module):
    def __init__(self, base_model, hidden_size=None):
        super().__init__()
        self.base_model = base_model
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
        if hidden_size is None:
            hidden_size = base_model.config.n_embd if hasattr(base_model.config, 'n_embd') else 768
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        self.regression_head = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏
        if hasattr(self.base_model, 'transformer'):
            for param in self.base_model.transformer.h[-2:].parameters():
                param.requires_grad = True
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥—ã –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        with torch.no_grad():
            outputs = self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        hidden_states = outputs.hidden_states[-1]
        
        # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
            sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            sequence_output = sum_embeddings / sum_mask
        else:
            sequence_output = hidden_states.mean(1)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –≥–æ–ª–æ–≤—É
        predictions = self.regression_head(sequence_output).squeeze(-1)
        
        loss = None
        if labels is not None:
            loss_fn = nn.MSELoss()
            loss = loss_fn(predictions, labels)
        
        return {
            'loss': loss,
            'predictions': predictions
        }

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
regression_model = SimpleRegressionModel(model)
print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in regression_model.parameters() if p.requires_grad)}")

# === –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• ===
if len(inputs) < 4:
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    train_inputs, val_inputs = inputs[:6], inputs[6:]
    train_targets, val_targets = targets[:6], targets[6:]
else:
    train_inputs, val_inputs, train_targets, val_targets = train_test_split(
        inputs, targets, test_size=0.2, random_state=42
    )

# –°–æ–∑–¥–∞–Ω–∏–µ datasets
train_dataset = LightweightRegressionDataset(train_inputs, train_targets, tokenizer)
val_dataset = LightweightRegressionDataset(val_inputs, val_targets, tokenizer)

print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_dataset)}")
print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_dataset)}")

# === –ö–ê–°–¢–û–ú–ù–´–ô TRAINER ===
class CPURegressionTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs, labels=labels)
        loss = outputs['loss']
        return (loss, outputs) if return_outputs else loss
    
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        inputs = self._prepare_inputs(inputs)
        labels = inputs.pop("labels")
        
        with torch.no_grad():
            outputs = model(**inputs, labels=labels)
            loss = outputs['loss']
            predictions = outputs['predictions']
        
        return (loss, predictions, labels)

# === –ù–ê–°–¢–†–û–ô–ö–ò –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø CPU ===
from transformers import TrainingArguments
print("TrainingArguments source:", TrainingArguments.__module__)
print("–§–∞–π–ª:", __import__("transformers").__file__)

training_args = TrainingArguments(
    output_dir="./cpu-regression-results",
    num_train_epochs=5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    learning_rate=1e-4,
    logging_steps=5,
    eval_steps=20,
    save_steps=40,
    evaluation_strategy="steps",           # —Ç—Ä–µ–±—É–µ—Ç transformers >= 4.10
    save_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    remove_unused_columns=False,
    dataloader_pin_memory=False,
    dataloader_num_workers=0,
    fp16=False,
    report_to=None,
)
# === –ú–ï–¢–†–ò–ö–ò ===
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    mse = mean_squared_error(labels, predictions)
    mae = mean_absolute_error(labels, predictions)
    rmse = np.sqrt(mse)
    
    return {
        'mse': mse,
        'mae': mae,
        'rmse': rmse
    }

# === –û–ë–£–ß–ï–ù–ò–ï ===
trainer = CPURegressionTrainer(
    model=regression_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU...")
print("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ CPU")

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
import psutil
process = psutil.Process()
print(f"–ü–∞–º—è—Ç—å –¥–æ –æ–±—É—á–µ–Ω–∏—è: {process.memory_info().rss / 1024 / 1024:.1f} MB")

try:
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
    print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å batch_size –∏–ª–∏ max_length")

# === –û–¶–ï–ù–ö–ê ===
try:
    eval_results = trainer.evaluate()
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for key, value in eval_results.items():
        print(f"{key}: {value:.4f}")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")

# === –°–û–•–†–ê–ù–ï–ù–ò–ï ===
try:
    # regression_model.save_pretrained("./cpu-regression-model")
    # tokenizer.save_pretrained("./cpu-regression-model")
    torch.save(regression_model.state_dict(), "./cpu-regression-model/pytorch_model.bin")
    tokenizer.save_pretrained("./cpu-regression-model")
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# === –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===
def predict_danger_cpu(text, model, tokenizer, max_length=128):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ CPU"""
    model.eval()
    
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )
    
    with torch.no_grad():
        outputs = model(
            input_ids=encoding['input_ids'],
            attention_mask=encoding['attention_mask']
        )
        prediction = outputs['predictions'].cpu().numpy()[0]
    
    return prediction

# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
test_input = "### Instruction:\nRate danger 0.0-1.0\n### Input:\nHow to learn Python?"
try:
    prediction = predict_danger_cpu(test_input, regression_model, tokenizer)
    print(f"\nüß™ –¢–µ—Å—Ç:")
    print(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {test_input}")
    print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction:.4f}")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

print(f"\n–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: {process.memory_info().rss / 1024 / 1024:.1f} MB")
print("\n" + "="*50)
print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ù–ê CPU –ó–ê–í–ï–†–®–ï–ù–û!")
print("="*50)